"""
Benchmark - Evaluation framework for AI agents.

Part of Layer 2: Intelligence Layer.
Measures agent performance across quality, security, and completeness.
"""

import json
import sys
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

# Set UTF-8 encoding for Windows console
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        pass


@dataclass
class TestCase:
    """Represents a benchmark test case."""
    id: str
    role: str  # PM, SA, DEV, QA, SECA
    name: str
    description: str
    input: str
    expected_output: Optional[str] = None
    expected_criteria: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> dict:
        return {
            "id": self.id,
            "role": self.role,
            "name": self.name,
            "description": self.description,
            "input": self.input,
            "expected_output": self.expected_output,
            "expected_criteria": self.expected_criteria,
            "metadata": self.metadata
        }


@dataclass
class EvaluationResult:
    """Result of an agent evaluation on a test case."""
    test_id: str
    role: str
    score: float  # 0.0 to 1.0
    feedback: str
    metrics: Dict[str, float]
    output: str
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    
    def to_dict(self) -> dict:
        return {
            "test_id": self.test_id,
            "role": self.role,
            "score": self.score,
            "feedback": self.feedback,
            "metrics": self.metrics,
            "output": self.output,
            "timestamp": self.timestamp
        }


class BenchmarkRunner:
    """
    Runs benchmarks to evaluate agent performance.
    
    Features:
    - Manage test cases per role
    - Automated scoring using Judge
    - Performance tracking over time
    - Regression detection
    """
    
    def __init__(self, storage_dir: Optional[Path] = None):
        self.storage_dir = storage_dir or Path(__file__).resolve().parent.parent.parent.parent / "docs" / ".benchmarks"
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        self.test_cases_file = self.storage_dir / "test_cases.json"
        self.results_file = self.storage_dir / "benchmark_history.json"
        
    def _load_test_cases(self) -> List[TestCase]:
        """Load benchmark test cases."""
        if not self.test_cases_file.exists():
            return self._create_default_test_cases()
        try:
            data = json.loads(self.test_cases_file.read_text(encoding='utf-8'))
            return [TestCase(**t) for t in data]
        except:
            return []
            
    def _create_default_test_cases(self) -> List[TestCase]:
        """Create a set of default test cases for core roles."""
        defaults = [
            TestCase(
                id="pm-01",
                role="PM",
                name="New Product Plan",
                description="Plan for a basic task management app",
                input="I want to build a simple task management app for personal use with login and cloud sync.",
                expected_criteria=["milestones", "risks", "resource allocation", "MVP definition"]
            ),
            TestCase(
                id="sa-01",
                role="SA",
                name="System Architecture Design",
                description="Architecture for a REST API with Auth",
                input="Design a scalable backend architecture for a task app with JWT auth and PostgreSQL.",
                expected_criteria=["database schema", "API endpoints", "security layer", "scalability"]
            ),
            TestCase(
                id="dev-01",
                role="DEV",
                name="API Implementation",
                description="Implement a simple CRUD API",
                input="Write a Python FastAPI service for task management (GET/POST/PUT/DELETE tasks).",
                expected_criteria=["FastAPI usage", "pydantic models", "error handling", "unit tests"]
            ),
            TestCase(
                id="qa-01",
                role="QA",
                name="Security & Quality Review",
                description="Find issues in a vulnerable script",
                input="Review this code for issues: def login(u, p): db.exec(f'SELECT * FROM users WHERE user=\"{u}\" AND pass=\"{p}\"')",
                expected_criteria=["SQL injection", "plain text password", "input validation"]
            )
        ]
        self._save_test_cases(defaults)
        return defaults

    def _save_test_cases(self, test_cases: List[TestCase]):
        """Save test cases to disk."""
        data = [t.to_dict() for t in test_cases]
        self.test_cases_file.write_text(json.dumps(data, indent=2), encoding='utf-8')

    def run_benchmark(self, role: Optional[str] = None) -> List[EvaluationResult]:
        """Run all test cases for a specific role or all roles."""
        test_cases = self._load_test_cases()
        if role:
            test_cases = [t for t in test_cases if t.role == role]
            
        print(f"ðŸš€ Running benchmark suite for {role if role else 'all roles'} ({len(test_cases)} cases)")
        
        results = []
        for test in test_cases:
            result = self.evaluate_agent(test)
            results.append(result)
            
        self._save_results(results)
        return results

    def evaluate_agent(self, test: TestCase) -> EvaluationResult:
        """
        Evaluate an agent on a specific test case.
        
        Note: Actual agent call should be handled outside or via callback.
        This provides a structured way to score the output.
        """
        # In a real scenario, we'd trigger the agent here.
        # For this implementation, we'll simulate the evaluation process.
        print(f"   ðŸ“‹ Evaluating {test.role} on test: {test.name}...")
        
        # Placeholder for agent execution logic
        # agent_output = trigger_agent(test.role, test.input)
        agent_output = f"[MOCK OUTPUT for {test.id}]"
        
        # Scoring logic using internal criteria evaluation
        metrics = {
            "completeness": 0.8,
            "correctness": 0.9,
            "readability": 0.85,
            "security": 0.7
        }
        
        score = sum(metrics.values()) / len(metrics)
        
        return EvaluationResult(
            test_id=test.id,
            role=test.role,
            score=score,
            feedback=f"Good performance on {test.name}, but criteria {test.expected_criteria[:1]} could be improved.",
            metrics=metrics,
            output=agent_output
        )

    def _save_results(self, results: List[EvaluationResult]):
        """Save benchmark results to history."""
        history = []
        if self.results_file.exists():
            try:
                history = json.loads(self.results_file.read_text(encoding='utf-8'))
            except:
                pass
                
        history.extend([r.to_dict() for r in results])
        self.results_file.write_text(json.dumps(history, indent=2), encoding='utf-8')

    def get_performance_stats(self) -> Dict[str, Any]:
        """Generate performance statistics and trends."""
        if not self.results_file.exists():
            return {"total_tests": 0}
            
        history = json.loads(self.results_file.read_text(encoding='utf-8'))
        
        stats = {
            "total_runs": len(history),
            "avg_score": sum(r["score"] for r in history) / len(history) if history else 0,
            "by_role": {}
        }
        
        for r in history:
            role = r["role"]
            if role not in stats["by_role"]:
                stats["by_role"][role] = {"count": 0, "total_score": 0, "history": []}
            
            stats["by_role"][role]["count"] += 1
            stats["by_role"][role]["total_score"] += r["score"]
            stats["by_role"][role]["history"].append(r["score"])
            
        for role in stats["by_role"]:
            stats["by_role"][role]["avg_score"] = stats["by_role"][role]["total_score"] / stats["by_role"][role]["count"]
            
        return stats


def main():
    """CLI entry point for benchmark runner."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Agent Benchmark - Evaluation Framework")
    subparsers = parser.add_subparsers(dest="command", help="Available commands")
    
    # Run benchmark
    run_parser = subparsers.add_parser("run", help="Run benchmark suite")
    run_parser.add_argument("--role", choices=["PM", "SA", "DEV", "QA", "SECA"], help="Test specific role")
    
    # Show stats
    subparsers.add_parser("stats", help="Show performance statistics")
    
    # List test cases
    subparsers.add_parser("list-tests", help="List all benchmark test cases")
    
    args = parser.parse_args()
    runner = BenchmarkRunner()
    
    if args.command == "run":
        results = runner.run_benchmark(role=args.role)
        print(f"\nðŸ“Š Benchmark results ({len(results)} tests):")
        for r in results:
            print(f"   - {r.test_id} ({r.role}): {r.score*100:.1f}% | {r.feedback[:50]}...")
            
    elif args.command == "stats":
        stats = runner.get_performance_stats()
        print("ðŸ§  Agent Performance Statistics:\n")
        print(f"   Total evaluations: {stats['total_runs']}")
        print(f"   Overall Avg Score: {stats['avg_score']*100:.1f}%")
        print("\n   By Role:")
        for role, data in stats["by_role"].items():
            print(f"      {role}: {data['avg_score']*100:.1f}% ({data['count']} tests)")
            
    elif args.command == "list-tests":
        tests = runner._load_test_cases()
        print(f"ðŸ“‹ Benchmark Test Cases ({len(tests)}):\n")
        for t in tests:
            print(f"   [{t.id}] {t.name} ({t.role})")
            print(f"      Input: {t.input[:60]}...")
            print()
            
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
